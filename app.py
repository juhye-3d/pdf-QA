import os
import streamlit as st
import requests
import io
import pdfplumber
from openai import OpenAI
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ------------------ PDF ì¶”ì¶œ í•¨ìˆ˜ ------------------
def get_pdf_text_with_plumber(url):
    try:
        response = requests.get(url)
        response.raise_for_status()
        content_type = response.headers.get("Content-Type", "")
        if not content_type.startswith("application/pdf"):
            return "[PDF ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨] ì´ ë§í¬ëŠ” PDF íŒŒì¼ì´ ì•„ë‹™ë‹ˆë‹¤."

        with io.BytesIO(response.content) as f:
            with pdfplumber.open(f) as pdf:
                text = ""
                for page in pdf.pages:
                    page_text = page.extract_text()
                    if page_text:
                        text += page_text + "\n"
                return text.strip()
    except Exception as e:
        return f"[PDF ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨] {e}"

# ------------------ Chunk ë¶„í•  í•¨ìˆ˜ ------------------
def split_text(text, chunk_size=3000, overlap=300):
    chunks = []
    start = 0
    while start < len(text):
        end = min(start + chunk_size, len(text))
        chunk = text[start:end]
        chunks.append(chunk.strip())
        start += chunk_size - overlap
    return chunks

# ------------------ TF-IDF ìœ ì‚¬ chunk ì¶”ì¶œ ------------------
def get_most_similar_chunks(query, chunks, top_n=2):
    corpus = chunks + [query]
    vectorizer = TfidfVectorizer().fit_transform(corpus)
    cosine_sim = cosine_similarity(vectorizer[-1], vectorizer[:-1])
    sim_scores = cosine_sim.flatten()
    top_indices = sim_scores.argsort()[::-1][:top_n]
    return [chunks[i] for i in top_indices]

# ------------------ GPT ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° ------------------
def stream_gpt_response(prompt: str):
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {"role": "system", "content": "ë‹¹ì‹ ì€ ì‹ ë¢°ë„ ë†’ì€ ë¬¸ì„œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=700,
        temperature=0.3,
        stream=True
    )
    full_response = ""
    for chunk in response:
        delta = chunk.choices[0].delta.get("content", "")
        full_response += delta
        yield delta
    return full_response

# ------------------ Streamlit ì‹œì‘ ------------------

st.set_page_config(page_title="ğŸ“„ PDF GPT Q&A", layout="centered")
st.title("ğŸ“„ PDF ê¸°ë°˜ GPT Q&A (ìµœëŒ€ 3íšŒ ì§ˆë¬¸)")

# ğŸ” ìƒíƒœ ì´ˆê¸°í™”
if "qa_round" not in st.session_state:
    st.session_state.qa_round = 1
    st.session_state.chat_history = []
    st.session_state.chunks = []
    st.session_state.document_loaded = False

# ğŸ“¥ PDF URL ì…ë ¥
pdf_url = st.text_input("ğŸ”— PDF ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš”", 
    value="https://arxiv.org/pdf/2501.00539.pdf")

if st.button("ğŸ“¥ PDF ë¶ˆëŸ¬ì˜¤ê¸°"):
    with st.spinner("ğŸ“„ PDFì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."):
        full_text = get_pdf_text_with_plumber(pdf_url)

    if full_text.startswith("[PDF ë¶ˆëŸ¬ì˜¤ê¸° ì‹¤íŒ¨]"):
        st.error(full_text)
    else:
        st.success("âœ… PDF ë¬¸ì„œ ë¶ˆëŸ¬ì˜¤ê¸° ì™„ë£Œ")
        st.session_state.chunks = split_text(full_text)
        st.session_state.qa_round = 1
        st.session_state.chat_history = []
        st.session_state.document_loaded = True
        st.text_area("ğŸ“„ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°", full_text[:1000], height=200)
        st.info(f"ğŸ“Œ ì´ {len(st.session_state.chunks)}ê°œ chunkë¡œ ë¶„í• ë¨")

# ğŸ” ì§ˆë¬¸ ì…ë ¥ & GPT ì‘ë‹µ (ìµœëŒ€ 3íšŒ)
if st.session_state.document_loaded and st.session_state.qa_round <= 3:

    question = st.text_input(
        f"â“ ì§ˆë¬¸ {st.session_state.qa_round}: GPTì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”", 
        key=f"question_{st.session_state.qa_round}"
    )

    if st.button(f"ğŸ” ì§ˆë¬¸ {st.session_state.qa_round} ì‹¤í–‰"):
        with st.spinner("ì§ˆë¬¸ê³¼ ê´€ë ¨ ìˆëŠ” chunkë¥¼ ì°¾ëŠ” ì¤‘..."):
            top_chunks = get_most_similar_chunks(question, st.session_state.chunks, top_n=2)
            context = "\n\n".join(top_chunks)
            prompt = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•´ì£¼ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {question}"

        st.markdown(f"#### ğŸ’¬ GPT ì‘ë‹µ {st.session_state.qa_round}:")
        placeholder = st.empty()
        response = ""
        for chunk in stream_gpt_response(prompt):
            response += chunk
            placeholder.markdown(response)

        # ê¸°ë¡ ì €ì¥
        st.session_state.chat_history.append((question, response))
        st.session_state.qa_round += 1

# â›” ì§ˆë¬¸ ì œí•œ ì´ˆê³¼ ì•ˆë‚´
if st.session_state.qa_round > 3:
    st.info("âœ… ìµœëŒ€ 3ë²ˆê¹Œì§€ ì§ˆë¬¸ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. í˜ì´ì§€ë¥¼ ìƒˆë¡œê³ ì¹¨í•˜ë©´ ë‹¤ì‹œ ì‹œì‘í•  ìˆ˜ ìˆì–´ìš”.")

# ğŸ“œ ì´ì „ ëŒ€í™” ë³´ê¸°
if st.session_state.chat_history:
    with st.expander("ğŸ“š ì´ì „ ì§ˆë¬¸ê³¼ ì‘ë‹µ ë³´ê¸°"):
        for i, (q, a) in enumerate(st.session_state.chat_history, start=1):
            st.markdown(f"**Q{i}: {q}**")
            st.markdown(f"**A{i}: {a}**")
